import os
import numpy as np
import pandas as pd
from tqdm import tqdm
import heig.input.dataset as ds
from heig.wgs.null import NullModel
from heig.wgs.relatedness import LOCOpreds
from heig.wgs.vsettest import VariantSetTest
from heig.wgs.wgs2 import prepare_vset, RV, RVsumstats
from heig.wgs.slidingwindow import SlidingWindow
from heig.wgs.utils import read_genotype_data, init_hail, get_temp_path, clean


"""
cluster inference

1. generate a bootstrap sample from null model
2. parse Z once, generate summary statistics
3. do analysis with sliding windows
4. count cluster size

input:
1. ldr + covar + bases
2. geno-mt, bfile or vcf
3. sig threshold
4. number of bootstrap samples
5. voxel-variant associations

output:
1. null distribution of cluster size 
2. screened voxel-variant associations

"""

class RVcluster:
    """
    Computing cluster size (i.e., number of associated voxels) 
    for variant sets in a bootstrap sample

    1. compute rare variant summary statistics
    2. sliding window analysis
    3. count cluster size for each sample-variant-set pair for multiple tests
    
    """
    def __init__(
            self, 
            null_model, 
            vset, 
            locus, 
            temp_path, 
            loco_preds=None
        ):
        """
        Parameters:
        ------------
        null_model: a NullModel instance
        vset: (m, n) csr_matrix of genotype
        locus: hail.Table including locus, maf, is_rare, grch37, variant_type, 
                chr, start, and end
        temp_path: a temporary path for saving interim data
        threads: number of threads
        loco_preds: a LOCOpreds instance of loco predictions
            loco_preds.data_reader(j) returns loco preds for chrj with matched subjects
        
        """
        self.null_model = null_model
        self.vset = vset
        self.locus = locus.cache()
        self.temp_path = temp_path
        self.loco_preds = loco_preds
        self.n_subs = self.null_model.resid_ldr.shape[0]

    def _generate_sample(self):
        """
        A bootstrap sample is generated by v_i*\\xi_{ij} for j = 1...r
        
        """
        rand_v = np.random.randn(self.n_sub).reshape(-1, 1)
        return rand_v
    
    def _compute_sumstats(self, rand_v):
        """
        A wrapper function for computing sumstats
        
        """
        rv_sumstats = RV(
            self.null_model.bases, 
            self.null_model.resid_ldr,
            self.null_model.covar,
            self.locus,
            self.loco_preds,
            rand_v
        )
        rv_sumstats.sumstats(self.vset, 500)
        rv_sumstats.save(f"{self.temp_path}_bootstrap")

    def _sliding_window(self):
        """
        A wrapper function for doing sliding window
        
        """
        rv_sumstats = RVsumstats(f"{self.temp_path}_bootstrap")
        rv_sumstats.calculate_var()
        vset_test = VariantSetTest(rv_sumstats.bases, rv_sumstats.var)

        sliding_window = SlidingWindow(rv_sumstats, window_length=1000, sliding_length=3000)
        n_windows = len(sliding_window.windows)
        all_pvalues = dict()
        for i, *results in tqdm(enumerate(sliding_window.parse_window_data()), total=n_windows, desc="Analyzing windows"):
            half_ldr_score, cov_mat, maf, is_rare, annot, chr, start, end = results[0]
            if half_ldr_score is not None:
                vset_test.input_vset(half_ldr_score, cov_mat, maf, is_rare, annot)
                pvalues = vset_test.do_inference(sliding_window.annot_cols)
                all_pvalues[f'window{i+1}'] = pvalues

        return all_pvalues

    def cluster_analysis(self):
        """
        The main function for computing cluster size for a bootstrap sample
        
        """
        rand_v = self._generate_sample()
        self._compute_sumstats(rand_v)
        all_pvalues = self._sliding_window()
        cluster_size_list = list()
        for _, pvalues in all_pvalues.items():
            cluster_size = (pvalues < 0.001).sum(axis=0).to_list()
            cluster_size_list.append(cluster_size)
        cluster_size_df = pd.DataFrame(np.array(cluster_size_list), columns=pvalues.columns)

        return cluster_size_df


def check_input(args, log):
    # if args.geno_mt is None and args.vcf is None and args.bfile is None:
    #     raise ValueError("one of --geno-mt, --vcf, or --bfile is required")
    if args.geno_mt is None:
        raise ValueError("--geno-mt is required. If you have bfile or vcf, convert it into a mt by --make-mt")
    if args.spark_conf is None:
        raise ValueError("--spark-conf is required")
    if args.null_model is None:
        raise ValueError("--null-model is required")
    
    if args.annot_cols is not None:
        args.annot_cols = args.annot_cols.split(",")
    if args.n_bootstrap is None:
        args.n_bootstrap = 100
        log.info('Set #bootstrap as 100')


def run(args, log):
    # checking if input is valid
    check_input(args, log)
    try:
        init_hail(args.spark_conf, args.grch37, args.out, log)

        # reading data and selecting LDRs
        log.info(f"Read null model from {args.null_model}")
        null_model = NullModel(args.null_model)
        null_model.select_ldrs(args.n_ldrs)

        # read loco preds
        if args.loco_preds is not None:
            log.info(f"Read LOCO predictions from {args.loco_preds}")
            loco_preds = LOCOpreds(args.loco_preds)
            if args.n_ldrs is not None:
                loco_preds.select_ldrs((0, args.n_ldrs))
            if loco_preds.ldr_col[1] - loco_preds.ldr_col[0] != null_model.n_ldrs:
                raise ValueError(
                    (
                        "inconsistent dimension in LDRs and LDR LOCO predictions. "
                        "Try to use --n-ldrs"
                    )
                )
            common_ids = ds.get_common_idxs(
                null_model.ids,
                loco_preds.ids,
                args.keep
            )
        else:
            common_ids = ds.get_common_idxs(null_model.ids, args.keep, single_id=True)
        common_ids = ds.remove_idxs(common_ids, args.remove, single_id=True)

        # read genotype data
        gprocessor = read_genotype_data(args, log)
    
        # do preprocessing
        log.info(f"Processing genetic data ...")
        gprocessor.extract_exclude_locus(args.extract_locus, args.exclude_locus)
        gprocessor.extract_chr_interval(args.chr_interval)
        gprocessor.keep_remove_idvs(common_ids)
        gprocessor.do_processing(mode="wgs")

        # extract and align subjects with the genotype data
        snps_mt_ids = gprocessor.subject_id()
        null_model.keep(snps_mt_ids)
        null_model.remove_dependent_columns()
        log.info(f"{len(snps_mt_ids)} common subjects in the data.")
        log.info(
            (f"{null_model.covar.shape[1]} fixed effects in the covariates (including the intercept) "
             "after removing redundant effects.\n")
        )

        if args.loco_preds is not None:
            loco_preds.keep(snps_mt_ids)
        else:
            loco_preds = None

        log.info('Preparing data ...')
        vset, locus = prepare_vset(gprocessor.snps_mt, gprocessor.variant_type)
        
        # wild bootstrap
        temp_path = get_temp_path(args.out)
        cluster = RVcluster(
            null_model, 
            vset, 
            locus, 
            temp_path, 
            loco_preds
        )

        for _ in tqdm(
            range(args.n_bootstrap), 
            desc=f"{args.n_bootstrap} bootstrap samples"
        ):
            null_cluster_size = cluster.cluster_analysis()
            with open(args.out + ".txt", "a") as file:
                file.write("\n".join(str(x) for x in null_cluster_size) + "\n")

        # save results
        log.info(f"\nSave null distribution of cluster size to {args.out}.txt")

    finally:
        if "temp_path" in locals():
            if os.path.exists(f"{temp_path}_bootstrap_rv_sumstats.h5"):
                os.remove(f"{temp_path}_bootstrap_rv_sumstats.h5")
            if os.path.exists(f"{temp_path}_bootstrap_locus_info.ht"):
                os.remove(f"{temp_path}_bootstrap_locus_info.ht")
        if 'loco_preds' in locals() and args.loco_preds is not None:
            loco_preds.close()
        
        clean(args.out)