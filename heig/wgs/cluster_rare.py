import time
import logging
import numpy as np
import hail as hl
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from scipy.sparse import csr_matrix
import heig.input.dataset as ds
from heig.wgs.null import NullModel
from heig.wgs.relatedness import LOCOpreds
from heig.wgs.vsettest import VariantSetTest
from heig.wgs.mt import SparseGenotype
from heig.wgs.wgs2 import SparseBandedLD
from heig.wgs.utils import *


"""
cluster inference

1. generate a bootstrap sample from null model
2. compute rv summary statistics
3. split summary statistics into random sets, permute sets?
4. do analysis using STAAR
5. count cluster size

input:
1. null model
2. sparse genotype
3. sig threshold
4. number of bootstrap samples

output:
1. null distribution of cluster size 

TODO:
1. choose other test: SKAT/burden
    
"""


class RVcluster:
    """
    Computing cluster size (i.e., number of associated voxels)
    for variant sets in a bootstrap sample

    1. compute rare variant summary statistics
    2. sliding window analysis
    3. count cluster size for each sample-variant-set pair for multiple tests

    """

    def __init__(
            self, 
            null_model, 
            vset, 
            locus, 
            maf, 
            is_rare, 
            sig_thresh=2.5e-6, 
            threads=1,
            loco_preds=None
        ):
        """
        Parameters:
        ------------
        null_model: a NullModel instance
        vset: (m, n) csr_matrix of genotype
        locus: hail.Table including locus, maf, is_rare, grch37, variant_type,
                chr, start, and end
        maf: a np.array of MAF
        is_rare: a np.array of boolean indices indicating MAC <= mac_threshold
        sig_thresh: significant threshold
        threads: number of threads
        loco_preds: a LOCOpreds instance of loco predictions
            loco_preds.data_reader(j) returns loco preds for chrj with matched subjects

        """
        self.null_model = null_model
        self.bases = self.null_model.bases.astype(np.float32)
        self.vset = vset
        self.locus = locus
        self.maf = maf
        self.is_rare = is_rare
        self.sig_thresh = sig_thresh
        self.threads = threads
        self.n_variants, self.n_subs = self.vset.shape
        self.n_covars = self.null_model.covar.shape[1]
        self.logger = logging.getLogger(__name__)
        
        covar_U, _, covar_Vt = np.linalg.svd(self.null_model.covar, full_matrices=False)
        half_covar_proj = np.dot(covar_U, covar_Vt).astype(np.float32)
        self.vset_half_covar_proj = self.vset @ half_covar_proj

        if loco_preds is not None:
            # extract chr
            chr = self.locus.aggregate(hl.agg.take(self.locus.locus.contig, 1)[0])
            if self.locus.locus.dtype.reference_genome.name == "GRCh38":
                chr = int(chr.replace("chr", ""))
            else:
                chr = int(chr)
            self.resid_ldr = self.null_model.resid_ldr - loco_preds.data_reader(chr)
        else:
            self.resid_ldr = self.null_model.resid_ldr

        self.vset_ld, self.block_size = self._get_sparse_ld_matrix()

    def _get_sparse_ld_matrix(self):
        positions = np.array(self.locus.locus.position.collect())
        ld = SparseBandedLD(self.vset, positions, 2000, self.threads)
        diag, data, row, col, shape = ld.data
        
        lower_row = col
        lower_col = row
        diag_row_col = np.arange(shape[0])

        full_row = np.concatenate([row, lower_row, diag_row_col])
        full_col = np.concatenate([col, lower_col, diag_row_col])
        full_data = np.concatenate([data, data, diag])

        vset_ld = csr_matrix((full_data, (full_row, full_col)), shape=shape)
        return vset_ld, ld.block_size

    def _compute_sumstats(self):
        """
        A bootstrap sample is generated by v_i*\\xi_{ij} for j = 1...r
        Each time partition new variant sets

        """
        rand_v = np.random.randn(self.n_subs).reshape(-1, 1)
        resid_ldr_rand = self.resid_ldr * rand_v
        inner_ldr = np.dot(resid_ldr_rand.T, resid_ldr_rand).astype(np.float32)
        self.var = np.sum(np.dot(self.bases, inner_ldr) * self.bases, axis=1)
        self.var /= self.n_subs - self.n_covars  # (N, )
        self.half_ldr_score = self.vset @ resid_ldr_rand 
        self.numeric_idx_list = self._partition_genome()

    def _variant_set_test(self):
        """
        A wrapper function of variant set test for multiple sets

        """
        # rv_sumstats = RVsumstats(f"{self.temp_path}_bootstrap")
        # rv_sumstats.calculate_var()
        # numeric_idx_list = self._partition_genome(rv_sumstats.block_size)

        # cluster_size_list = list() 
        # vset_test = VariantSetTest(rv_sumstats.bases, rv_sumstats.var)
        # for numeric_idx in numeric_idx_list:
        #     half_ldr_score, cov_mat, maf, is_rare = rv_sumstats.parse_data(
        #         numeric_idx
        #     )
        #     if half_ldr_score is None:
        #         continue
        #     vset_test.input_vset(half_ldr_score, cov_mat, maf, is_rare, None)
        #     pvalues = vset_test.do_inference()
        #     cluster_size = (pvalues["STAAR-O"] < self.sig_thresh).sum() 
        #     cluster_size_list.append(cluster_size)

        cluster_size_list = []
    
        with ThreadPoolExecutor(max_workers=self.threads) as executor:
            futures = [
                executor.submit(self._variant_set_test_, numeric_idx)
                for numeric_idx in self.numeric_idx_list
            ]
            
            for future in futures:
                result = future.result()
                if result is not None:
                    cluster_size_list.append(result)
        
        return cluster_size_list

    def _variant_set_test_(self, numeric_idx):
        """
        Testing a single variant set
        
        """
        vset_test = VariantSetTest(self.bases, self.var)
        half_ldr_score, cov_mat, maf, is_rare = self._parse_data(
            numeric_idx
        )
        if half_ldr_score is None or np.sum(maf * self.n_subs * 2) < 10:
            return None

        vset_test.input_vset(half_ldr_score, cov_mat, maf, is_rare, None)
        pvalues = vset_test.do_inference()
        cluster_size = (pvalues["STAAR-O"] < self.sig_thresh).sum()

        return cluster_size
    
    def _parse_data(self, numeric_idx):
        """
        Extracting data for a variant set to test
        
        """
        if numeric_idx[-1] - numeric_idx[0] >= self.block_size[numeric_idx[0]]:
            return None, None, None, None
        half_ldr_score = np.array(self.half_ldr_score[numeric_idx])
        vset_half_covar_proj = np.array(self.vset_half_covar_proj[numeric_idx])
        vset_ld = self.vset_ld[numeric_idx][:, numeric_idx]
        cov_mat = np.array((vset_ld - vset_half_covar_proj @ vset_half_covar_proj.T))
        maf = self.maf[numeric_idx]
        is_rare = self.is_rare[numeric_idx]

        return half_ldr_score, cov_mat, maf, is_rare
        
    def _partition_genome(self):
        """
        randomly partition genotype data into intervals with length 10-30
        
        """
        # numeric_idx_list = list()
        # left = 0
        # right = 0
        # while right < n_variants:
        #     right += np.random.choice(list(range(10, 30)), 1)[0]
        #     numeric_idx_list.append(list(range(left, min(right, n_variants))))
        #     left = right + 10
        #     right += 10

        numeric_idx_list = list()
        n_variants = len(self.block_size) - 1
        left = 0
        while left < n_variants:
            if self.block_size[left] > 10:
                max_interval_len = min(30, self.block_size[left])
                right = left + np.random.choice(list(range(10, max_interval_len)), 1)[0]
                numeric_idx_list.append(list(range(left, min(right, n_variants))))
                left = right + 10
            else:
                left += 1
        mean_length = int(np.mean([len(numeric_idx) for numeric_idx in numeric_idx_list]))
        self.logger.info(f"{len(numeric_idx_list)} variant sets (mean size {mean_length}) to analyze.")

        return numeric_idx_list

    def cluster_analysis(self):
        """
        The main function for computing cluster size for a bootstrap sample

        """
        self._compute_sumstats()
        cluster_size_list = self._variant_set_test()

        return cluster_size_list


def check_input(args, log):
    if args.sparse_genotype is None:
        raise ValueError("--sparse-genotype is required")
    if args.spark_conf is None:
        raise ValueError("--spark-conf is required")
    if args.null_model is None:
        raise ValueError("--null-model is required")
    if args.n_bootstrap is None:
        args.n_bootstrap = 50
        log.info("Set #bootstrap as 50")
    if args.mac_thresh is None:
        args.mac_thresh = 10
        log.info(f"Set --mac-thresh as default 10")
    elif args.mac_thresh < 0:
        raise ValueError("--mac-thresh must be greater than 0")
    if args.sig_thresh is None:
        args.sig_thresh = 2.5e-6
        log.info("Set significance threshold as 2.5e-6")


def run(args, log):
    # checking if input is valid
    check_input(args, log)
    try:
        init_hail(args.spark_conf, args.grch37, args.out, log)

        # reading data and selecting LDRs
        log.info(f"Read null model from {args.null_model}")
        null_model = NullModel(args.null_model)
        null_model.select_ldrs(args.n_ldrs)
        null_model.select_voxels(args.voxels)

        # reading sparse genotype data
        sparse_genotype = SparseGenotype(args.sparse_genotype, args.mac_thresh)
        log.info(f"Read sparse genotype data from {args.sparse_genotype}")
        log.info(f"{sparse_genotype.vset.shape[1]} subjects and {sparse_genotype.vset.shape[0]} variants.")

        # read loco preds
        if args.loco_preds is not None:
            log.info(f"Read LOCO predictions from {args.loco_preds}")
            loco_preds = LOCOpreds(args.loco_preds)
            if args.n_ldrs is not None:
                loco_preds.select_ldrs((0, args.n_ldrs))
            if loco_preds.ldr_col[1] - loco_preds.ldr_col[0] != null_model.n_ldrs:
                raise ValueError(
                    (
                        "inconsistent dimension in LDRs and LDR LOCO predictions. "
                        "Try to use --n-ldrs"
                    )
                )
            common_ids = ds.get_common_idxs(
                sparse_genotype.ids.index,
                null_model.ids,
                loco_preds.ids,
                args.keep,
            )
        else:
            common_ids = ds.get_common_idxs(
                sparse_genotype.ids.index, null_model.ids, args.keep
            )
        common_ids = ds.remove_idxs(common_ids, args.remove)

        # log.info(f"Processing sparse genetic data ...")
        if args.extract_locus is not None:
            args.extract_locus = read_extract_locus(args.extract_locus, args.grch37, log)
        if args.exclude_locus is not None:
            args.exclude_locus = read_exclude_locus(args.exclude_locus, args.grch37, log)
        
        sparse_genotype.keep(common_ids)
        sparse_genotype.extract_exclude_locus(args.extract_locus, args.exclude_locus)
        sparse_genotype.extract_chr_interval(args.chr_interval)
        sparse_genotype.extract_maf(args.maf_min, args.maf_max)

        # extract and align subjects with the genotype data
        null_model.keep(common_ids)
        null_model.remove_dependent_columns()
        log.info(f"{len(common_ids)} common subjects in the data.")
        log.info(
            (
                f"{null_model.covar.shape[1]} fixed effects in the covariates (including the intercept) "
                "after removing redundant effects.\n"
            )
        )

        if args.loco_preds is not None:
            loco_preds.keep(common_ids)
        else:
            loco_preds = None

        vset, locus, maf, is_rare = sparse_genotype.parse_data()
        log.info(f"Using {vset.shape[0]} variants in wild bootstrap ...")

        # wild bootstrap
        # temp_path = get_temp_path(args.out)
        cluster = RVcluster(
            null_model, 
            vset, 
            locus, 
            maf, 
            is_rare, 
            args.sig_thresh, 
            args.threads, 
            loco_preds
        )

        for i in tqdm(
            range(args.n_bootstrap), desc=f"{args.n_bootstrap} bootstrap samples"
        ):
            log.info(f"Doing bootstrap sample {i+1} ...")
            start_time = time.time()
            cluster_size_list = cluster.cluster_analysis()
            with open(args.out + ".txt", "a") as file:
                file.write("\n".join(str(x) for x in cluster_size_list) + "\n")
            elapsed_time = int((time.time() - start_time) * 1000)
            log.info(f"done ({elapsed_time}ms)")

        # save results
        log.info(f"\nSaved null distribution of cluster size to {args.out}.txt")

    finally:
        # if "temp_path" in locals():
        #     if os.path.exists(f"{temp_path}_bootstrap_rv_sumstats.h5"):
        #         os.remove(f"{temp_path}_bootstrap_rv_sumstats.h5")
        #     if os.path.exists(f"{temp_path}_bootstrap_locus_info.ht"):
        #         shutil.rmtree(f"{temp_path}_bootstrap_locus_info.ht")
        if "loco_preds" in locals() and args.loco_preds is not None:
            loco_preds.close()

        clean(args.out)
