import os
import time
import shutil
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import heig.input.dataset as ds
from heig.wgs.null import NullModel
from heig.wgs.relatedness import LOCOpreds
from heig.wgs.vsettest import VariantSetTest
from heig.wgs.mt import SparseGenotype
from heig.wgs.wgs2 import RV, RVsumstats
from heig.wgs.utils import init_hail, get_temp_path, clean


"""
cluster inference

1. generate a bootstrap sample from null model
2. compute rv summary statistics
3. split summary statistics into random sets, permute sets?
4. do analysis using STAAR
5. count cluster size

input:
1. null model
2. sparse genotype
3. sig threshold
4. number of bootstrap samples

output:
1. null distribution of cluster size 

TODO:
1. choose other test: SKAT/burden
    
"""


class RVcluster:
    """
    Computing cluster size (i.e., number of associated voxels)
    for variant sets in a bootstrap sample

    1. compute rare variant summary statistics
    2. sliding window analysis
    3. count cluster size for each sample-variant-set pair for multiple tests

    """

    def __init__(
            self, 
            null_model, 
            vset, 
            locus, 
            maf, 
            is_rare, 
            temp_path, 
            sig_thresh=0.0001, 
            threads=1,
            loco_preds=None
        ):
        """
        Parameters:
        ------------
        null_model: a NullModel instance
        vset: (m, n) csr_matrix of genotype
        locus: hail.Table including locus, maf, is_rare, grch37, variant_type,
                chr, start, and end
        maf: a np.array of MAF
        is_rare: a np.array of boolean indices indicating MAC <= mac_threshold
        temp_path: a temporary path for saving interim data
        sig_thresh: significant threshold
        threads: number of threads
        loco_preds: a LOCOpreds instance of loco predictions
            loco_preds.data_reader(j) returns loco preds for chrj with matched subjects

        """
        self.null_model = null_model
        self.vset = vset
        self.locus = locus
        self.maf = maf
        self.is_rare = is_rare
        self.temp_path = temp_path
        self.sig_thresh = sig_thresh
        self.threads = threads
        self.loco_preds = loco_preds
        self.n_subs = self.null_model.resid_ldr.shape[0]
        self.n_variants = vset.shape[0]

    def _generate_sample(self):
        """
        A bootstrap sample is generated by v_i*\\xi_{ij} for j = 1...r

        """
        rand_v = np.random.randn(self.n_subs).reshape(-1, 1)
        return rand_v

    def _compute_sumstats(self, rand_v):
        """
        A wrapper function for computing sumstats

        """
        rv_sumstats = RV(
            self.null_model.bases,
            self.null_model.resid_ldr,
            self.null_model.covar,
            self.locus,
            self.maf,
            self.is_rare,
            self.loco_preds,
            rand_v,
        )
        rv_sumstats.sumstats(self.vset, 50)
        rv_sumstats.save(f"{self.temp_path}_bootstrap")

    def _variant_set_test(self):
        """
        A wrapper function of variant set test for multiple sets

        """
        rv_sumstats = RVsumstats(f"{self.temp_path}_bootstrap")
        rv_sumstats.calculate_var()
        numeric_idx_list = self._partition_genome(self.n_variants)

        # cluster_size_list = list() 
        # vset_test = VariantSetTest(rv_sumstats.bases, rv_sumstats.var)
        # for numeric_idx in numeric_idx_list:
        #     half_ldr_score, cov_mat, maf, is_rare = rv_sumstats.parse_data(
        #         numeric_idx
        #     )
        #     if half_ldr_score is None:
        #         continue
        #     vset_test.input_vset(half_ldr_score, cov_mat, maf, is_rare, None)
        #     pvalues = vset_test.do_inference()
        #     cluster_size = (pvalues["STAAR-O"] < self.sig_thresh).sum() 
        #     cluster_size_list.append(cluster_size)

        cluster_size_list = []
    
        with ThreadPoolExecutor(max_workers=self.threads) as executor:
            futures = [
                executor.submit(self._variant_set_test_, rv_sumstats, numeric_idx)
                for numeric_idx in numeric_idx_list
            ]
            
            for future in futures:
                result = future.result()
                if result is not None:
                    cluster_size_list.append(result)
        
        return cluster_size_list

    def _variant_set_test_(self, rv_sumstats, numeric_idx):
        """
        Testing a single variant set
        
        """
        vset_test = VariantSetTest(rv_sumstats.bases, rv_sumstats.var)
        half_ldr_score, cov_mat, maf, is_rare = rv_sumstats.parse_data(
            numeric_idx
        )
        if half_ldr_score is None:
            return None
        if np.sum(maf * rv_sumstats.n_subs * 2) < 10:
            return None
        vset_test.input_vset(half_ldr_score, cov_mat, maf, is_rare, None)
        pvalues = vset_test.do_inference()
        cluster_size = (pvalues["STAAR-O"] < self.sig_thresh).sum()

        return cluster_size

        
    @staticmethod
    def _partition_genome(n_variants):
        """
        randomly partition list(range(n_variants)) into intervals with length 2-20
        
        """
        numeric_idx_list = list()
        left = 0
        right = 0
        while right < n_variants:
            right += np.random.choice(list(range(10, 50)), 1)[0]
            numeric_idx_list.append(list(range(left, min(right, n_variants))))
            left = right + 50
            right += 50
            
        return numeric_idx_list

    def cluster_analysis(self):
        """
        The main function for computing cluster size for a bootstrap sample

        """
        rand_v = self._generate_sample()
        self._compute_sumstats(rand_v)
        cluster_size_list = self._variant_set_test()

        return cluster_size_list


def check_input(args, log):
    if args.sparse_genotype is None:
        raise ValueError("--sparse-genotype is required")
    if args.spark_conf is None:
        raise ValueError("--spark-conf is required")
    if args.null_model is None:
        raise ValueError("--null-model is required")
    if args.n_bootstrap is None:
        args.n_bootstrap = 50
        log.info("Set #bootstrap as 50")
    if args.mac_thresh is None:
        args.mac_thresh = 10
        log.info(f"Set --mac-thresh as default 10")
    elif args.mac_thresh < 0:
        raise ValueError("--mac-thresh must be greater than 0")
    if args.sig_thresh is None:
        args.sig_thresh = 0.0001
        log.info("Set significance threshold as 0.0001")


def run(args, log):
    # checking if input is valid
    check_input(args, log)
    try:
        init_hail(args.spark_conf, args.grch37, args.out, log)

        # reading data and selecting LDRs
        log.info(f"Read null model from {args.null_model}")
        null_model = NullModel(args.null_model)
        null_model.select_ldrs(args.n_ldrs)
        null_model.select_voxels(args.voxels)

        # reading sparse genotype data
        sparse_genotype = SparseGenotype(args.sparse_genotype, args.mac_thresh)
        log.info(f"Read sparse genotype data from {args.sparse_genotype}")
        log.info(f"{sparse_genotype.vset.shape[1]} subjects and {sparse_genotype.vset.shape[0]} variants.")

        # read loco preds
        if args.loco_preds is not None:
            log.info(f"Read LOCO predictions from {args.loco_preds}")
            loco_preds = LOCOpreds(args.loco_preds)
            if args.n_ldrs is not None:
                loco_preds.select_ldrs((0, args.n_ldrs))
            if loco_preds.ldr_col[1] - loco_preds.ldr_col[0] != null_model.n_ldrs:
                raise ValueError(
                    (
                        "inconsistent dimension in LDRs and LDR LOCO predictions. "
                        "Try to use --n-ldrs"
                    )
                )
            common_ids = ds.get_common_idxs(
                sparse_genotype.ids.index,
                null_model.ids,
                loco_preds.ids,
                args.keep,
            )
        else:
            common_ids = ds.get_common_idxs(
                sparse_genotype.ids.index, null_model.ids, args.keep
            )
        common_ids = ds.remove_idxs(common_ids, args.remove)

        # log.info(f"Processing sparse genetic data ...")
        sparse_genotype.keep(common_ids)
        sparse_genotype.extract_exclude_locus(args.extract_locus, args.exclude_locus)
        sparse_genotype.extract_chr_interval(args.chr_interval)
        sparse_genotype.extract_maf(args.maf_min, args.maf_max)

        # extract and align subjects with the genotype data
        null_model.keep(common_ids)
        null_model.remove_dependent_columns()
        log.info(f"{len(common_ids)} common subjects in the data.")
        log.info(
            (
                f"{null_model.covar.shape[1]} fixed effects in the covariates (including the intercept) "
                "after removing redundant effects.\n"
            )
        )

        if args.loco_preds is not None:
            loco_preds.keep(common_ids)
        else:
            loco_preds = None

        vset, locus, maf, is_rare = sparse_genotype.parse_data()
        log.info(f"Using {vset.shape[0]} variants in wild bootstrap ...")

        # wild bootstrap
        temp_path = get_temp_path(args.out)
        cluster = RVcluster(
            null_model, 
            vset, 
            locus, 
            maf, 
            is_rare, 
            temp_path, 
            args.sig_thresh, 
            args.threads, 
            loco_preds
        )

        for i in tqdm(
            range(args.n_bootstrap), desc=f"{args.n_bootstrap} bootstrap samples"
        ):
            log.info(f"Doing bootstrap sample {i+1} ...")
            start_time = time.time()
            cluster_size_list = cluster.cluster_analysis()
            with open(args.out + ".txt", "a") as file:
                file.write("\n".join(str(x) for x in cluster_size_list) + "\n")
            elapsed_time = int((time.time() - start_time) * 1000)
            log.info(f"done ({elapsed_time}ms)")

        # save results
        log.info(f"\nSave null distribution of cluster size to {args.out}.txt")

    finally:
        if "temp_path" in locals():
            if os.path.exists(f"{temp_path}_bootstrap_rv_sumstats.h5"):
                os.remove(f"{temp_path}_bootstrap_rv_sumstats.h5")
            if os.path.exists(f"{temp_path}_bootstrap_locus_info.ht"):
                shutil.rmtree(f"{temp_path}_bootstrap_locus_info.ht")
        if "loco_preds" in locals() and args.loco_preds is not None:
            loco_preds.close()

        clean(args.out)
