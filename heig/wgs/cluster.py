import os
import time
import shutil
import numpy as np
import pandas as pd
from tqdm import tqdm
from scipy.stats import chi2
import heig.input.dataset as ds
from heig.wgs.gwas import DoGWAS
from heig.wgs.null import fit_null_model
from heig.wgs.relatedness import LOCOpreds
from heig.wgs.utils import read_genotype_data, init_hail, get_temp_path, clean
from heig.sumstats import GWASHEIG, read_sumstats
from heig.voxelgwas import VGWAS, write_header, voxel_reader, process_voxels


"""
cluster inference

input:
1. ldr + covar + bases
2. geno-mt, bfile or vcf
3. sig threshold
4. number of bootstrap samples
5. voxel-variant associations

output:
1. null distribution of cluster size 
2. screened voxel-variant associations

"""


class Cluster:
    """
    Computing cluster size (i.e., number of associated voxels)
    for SNPs in a bootstrap sample

    1. LDR GWAS
    2. LDR summary statistics process
    3. voxel-level GWAS reconstruction
    4. count cluster size for each sample-SNP pair

    """

    def __init__(
        self,
        gprocessor,
        resid_ldrs,
        covar,
        bases,
        voxels,
        temp_path,
        sig_thresh=0.00001,
        threads=1,
        loco_preds=None,
    ):
        """
        Parameters:
        ------------
        gprocessor: a GProcessor instance including hail.MatrixTable
        resid_ldrs: a pd.DataFrame of LDR residuals with a single index 'IID'
        covar: a pd.DataFrame of covariates with a single index 'IID'
        bases: a np.array of bases
        voxels: a np.array of voxels numeric indices
        temp_path: a temporary path for saving interim data
        sig_thresh: significant threshold
        threads: number of threads
        loco_preds: a LOCOpreds instance of loco predictions
            loco_preds.data_reader(j) returns loco preds for chrj with matched subjects

        """
        self.gprocessor = gprocessor
        self.resid_ldrs = resid_ldrs
        self.covar = covar
        self.n = covar.shape[0]
        self.n_snps = None
        self.bases = bases
        self.voxels = voxels
        self.temp_path = temp_path
        self.thresh_chisq = chi2.ppf(1 - sig_thresh, 1)
        self.threads = threads
        self.loco_preds = loco_preds
        self.n_subs = resid_ldrs.shape[0]
        self.cols_map, self.cols_map2 = self._map_cols()

        self.is_valid_snp, self.snpinfo = None, None

    @staticmethod
    def _map_cols():
        """
        Creating two dicts for mapping provided colnames and standard colnames

        Returns:
        ---------
        cols_map: keys are standard colnames, values are provided colnames
        cols_map2: keys are provided colnames, values are standard colnames

        """
        cols_map = dict()
        cols_map["N"] = "n_called"
        cols_map["CHR"] = "chr"
        cols_map["POS"] = "pos"
        cols_map["SNP"] = "rsid"
        cols_map["EFFECT"] = "beta"
        cols_map["null_value"] = 0
        cols_map["SE"] = "standard_error"
        cols_map["A1"] = "alt_allele"
        cols_map["A2"] = "ref_allele"
        cols_map["Z"] = "t_stat"

        cols_map2 = dict()
        for k, v in cols_map.items():
            if v is not None and k not in ("n", "maf_min", "info_min", "null_value"):
                cols_map2[v] = k

        return cols_map, cols_map2

    def _generate_sample(self):
        """
        A bootstrap sample is generated by v_i*\\xi_{ij} for j = 1...r

        """
        rand_v = np.random.randn(self.n_subs).reshape(-1, 1)
        return rand_v

    def _do_ldr_gwas(self, rand_v):
        """
        A wrapper function for doing LDR GWAS

        """
        ldr_gwas = DoGWAS(
            self.gprocessor,
            self.resid_ldrs,
            self.covar,
            self.temp_path,
            self.loco_preds,
            rand_v,
        )
        ldr_gwas.save(f"{self.temp_path}_bootstrap_ldr_gwas")

    def _process_sumstats(self):
        """
        A wrapper function for processing LDR sumstats

        """
        sumstats = GWASHEIG(
            [f"{self.temp_path}_bootstrap_ldr_gwas.parquet"],
            self.cols_map,
            self.cols_map2,
            f"{self.temp_path}_bootstrap_ldr_sumstats",
        )
        if self.is_valid_snp is None and self.snpinfo is None:
            self.is_valid_snp, self.snpinfo = sumstats.process(
                self.threads, self.is_valid_snp, self.snpinfo
            )
        else:
            sumstats.process(self.threads, self.is_valid_snp, self.snpinfo)

    def _do_vgwas(self, rand_v):
        """
        A wrapper function for voxel-level GWAS reconstruction

        """
        rand_resid_ldrs = self.resid_ldrs.values * rand_v
        ldr_cov = np.dot(rand_resid_ldrs.T, rand_resid_ldrs) / self.n
        sumstats = read_sumstats(f"{self.temp_path}_bootstrap_ldr_sumstats")
        ldr_n = np.array(sumstats.snpinfo["N"]).reshape(-1, 1)
        snp_idxs = np.ones(sumstats.n_snps, dtype=bool)
        self.n_snps = sumstats.n_snps
        write_header(sumstats.snpinfo, f"{self.temp_path}_bootstrap_vgwas.txt")
        vgwas = VGWAS(self.bases, ldr_cov, sumstats, snp_idxs, ldr_n, self.threads)

        for voxel_idxs in voxel_reader(np.sum(snp_idxs), self.voxels):
            voxel_beta = vgwas.recover_beta(voxel_idxs, self.threads)
            voxel_se = vgwas.recover_se(voxel_idxs, voxel_beta)
            voxel_z = voxel_beta / voxel_se
            all_sig_idxs = voxel_z * voxel_z >= self.thresh_chisq
            all_sig_idxs_voxel = all_sig_idxs.any(axis=0)

            process_voxels(
                voxel_idxs,
                all_sig_idxs,
                sumstats.snpinfo,
                voxel_beta,
                voxel_se,
                voxel_z,
                all_sig_idxs_voxel,
                f"{self.temp_path}_bootstrap_vgwas.txt",
                self.threads,
            )

    def cluster_analysis(self):
        """
        The main function for computing cluster size for a bootstrap sample

        """
        rand_v = self._generate_sample()
        self._do_ldr_gwas(rand_v)
        self._process_sumstats()
        self._do_vgwas(rand_v)

        vgwas = pd.read_csv(f"{self.temp_path}_bootstrap_vgwas.txt", sep="\t")
        null_cluster_size = list(vgwas["SNP"].value_counts())
        null_cluster_size += [0] * (self.n_snps - len(null_cluster_size))

        return null_cluster_size


def calculate_resid_ldrs(ldrs, covar):
    """
    Calculating LDR residuals

    Parameters:
    ------------
    resid_ldrs: a pd.DataFrame of LDR residuals with a single index 'IID'
    covar: a pd.DataFrame of covariates with a single index 'IID'

    """
    ldrs_array = ldrs.values
    covar_array = covar.values
    resid_ldrs = fit_null_model(covar_array, ldrs_array)
    resid_ldrs = pd.DataFrame(resid_ldrs)
    resid_ldrs.index = ldrs.index

    return resid_ldrs


def check_input(args, log):
    # required arguments
    if args.ldrs is None:
        raise ValueError("--ldrs is required")
    if args.covar is None:
        raise ValueError("--covar is required")
    if args.spark_conf is None:
        raise ValueError("--spark-conf is required")
    if args.geno_mt is None:
        raise ValueError(
            "--geno-mt is required. If you have bfile or vcf, convert it into a mt by --make-mt"
        )
    if args.bases is None:
        raise ValueError("--bases is required")
    if args.ldr_cov is None:
        raise ValueError("--ldr-cov is required")
    if args.sig_thresh is None:
        args.sig_thresh = 0.00001
        log.info("Set significance threshold as 0.00001")
    if args.n_bootstrap is None:
        args.n_bootstrap = 50
        log.info("Set #bootstrap as 50")


def run(args, log):
    check_input(args, log)
    try:
        init_hail(args.spark_conf, args.grch37, args.out, log)

        # read LDRs, bases, ldr_cov, and covariates
        log.info(f"Read LDRs from {args.ldrs}")
        ldrs = ds.Dataset(args.ldrs)
        log.info(f"{ldrs.data.shape[1]} LDRs and {ldrs.data.shape[0]} subjects.")
        ldr_cov = np.load(args.ldr_cov)
        log.info(f"Read variance-covariance matrix of LDRs from {args.ldr_cov}")
        bases = np.load(args.bases)
        log.info(f"{bases.shape[1]} bases read from {args.bases}")
        log.info(f"Read covariates from {args.covar}")
        covar = ds.Covar(args.covar, args.cat_covar_list)

        # keep selected LDRs
        if args.n_ldrs is not None:
            bases, ldr_cov, _, ldrs.data = ds.keep_ldrs(
                args.n_ldrs, bases, ldr_cov, resid_ldrs=ldrs.data
            )
            log.info(f"Keep the top {args.n_ldrs} LDRs.")

        # check numbers of LDRs are the same
        if bases.shape[1] != ldr_cov.shape[0] or bases.shape[1] != ldrs.data.shape[1]:
            raise ValueError(
                (
                    "inconsistent dimension in bases, variance-covariance matrix of LDRs, "
                    "and LDRs. "
                    "Try to use --n-ldrs"
                )
            )

        # read loco preds
        if args.loco_preds is not None:
            log.info(f"Read LOCO predictions from {args.loco_preds}")
            loco_preds = LOCOpreds(args.loco_preds)
            if args.n_ldrs is not None:
                loco_preds.select_ldrs((0, args.n_ldrs))
            if loco_preds.ldr_col[1] - loco_preds.ldr_col[0] != ldrs.data.shape[1]:
                raise ValueError(
                    (
                        "inconsistent dimension in LDRs and LDR LOCO predictions. "
                        "Try to use --n-ldrs"
                    )
                )
            common_ids = ds.get_common_idxs(
                ldrs.data.index,
                covar.data.index,
                loco_preds.ids,
                args.keep,
            )
        else:
            # keep subjects
            common_ids = ds.get_common_idxs(
                ldrs.data.index, covar.data.index, args.keep
            )
        common_ids = ds.remove_idxs(common_ids, args.remove, single_id=True)

        # read genotype data
        gprocessor = read_genotype_data(args, log)

        log.info(f"Processing genetic data ...")
        gprocessor.extract_exclude_snps(args.extract, args.exclude)
        gprocessor.extract_chr_interval(args.chr_interval)
        gprocessor.keep_remove_idvs(common_ids)
        gprocessor.do_processing(mode="gwas")

        # extract common subjects and align data
        snps_mt_ids = gprocessor.subject_id()
        ldrs.to_single_index()
        covar.to_single_index()
        ldrs.keep_and_remove(snps_mt_ids)
        covar.keep_and_remove(snps_mt_ids)
        covar.cat_covar_intercept()

        if args.loco_preds is not None:
            loco_preds.keep(snps_mt_ids)
        else:
            loco_preds = None
        log.info(f"{len(snps_mt_ids)} common subjects in the data.")
        log.info(
            f"{covar.data.shape[1]} fixed effects in the covariates (including the intercept)."
        )

        # fit null model
        resid_ldrs = calculate_resid_ldrs(ldrs.data, covar.data)

        if args.voxels is not None:
            if np.max(args.voxels) + 1 <= bases.shape[0] and np.min(args.voxels) >= 0:
                log.info(f"{len(args.voxels)} voxels included.")
            else:
                raise ValueError("--voxels index (one-based) out of range")
        else:
            args.voxels = np.arange(bases.shape[0])

        # wild bootstrap
        temp_path = get_temp_path(args.out)
        gprocessor.cache()
        cluster = Cluster(
            gprocessor,
            resid_ldrs,
            covar.data,
            bases,
            args.voxels,
            temp_path,
            args.sig_thresh,
            args.threads,
            loco_preds,
        )
        for i in tqdm(
            range(args.n_bootstrap), desc=f"{args.n_bootstrap} bootstrap samples"
        ):
            log.info(f"Doing bootstrap sample {i+1} ...")
            start_time = time.time()
            null_cluster_size = cluster.cluster_analysis()
            with open(args.out + ".txt", "a") as file:
                file.write("\n".join(str(x) for x in null_cluster_size) + "\n")
            elapsed_time = int((time.time() - start_time) * 1000)
            log.info(f"done ({elapsed_time}ms)")

        # save results
        log.info(f"\nSave null distribution of cluster size to {args.out}.txt")

    finally:
        if "temp_path" in locals():
            if os.path.exists(f"{temp_path}_covar.txt"):
                os.remove(f"{temp_path}_covar.txt")
            if os.path.exists(f"{temp_path}_ldr.txt"):
                os.remove(f"{temp_path}_ldr.txt")
            if os.path.exists(f"{temp_path}_bootstrap_ldr_gwas.parquet"):
                shutil.rmtree(f"{temp_path}_bootstrap_ldr_gwas.parquet")
            if os.path.exists(f"{temp_path}_bootstrap_ldr_sumstats.sumstats"):
                os.remove(f"{temp_path}_bootstrap_ldr_sumstats.sumstats")
            if os.path.exists(f"{temp_path}_bootstrap_ldr_sumstats.snpinfo"):
                os.remove(f"{temp_path}_bootstrap_ldr_sumstats.snpinfo")
            if os.path.exists(f"{temp_path}_bootstrap_vgwas.txt"):
                os.remove(f"{temp_path}_bootstrap_vgwas.txt")
        if "loco_preds" in locals() and args.loco_preds is not None:
            loco_preds.close()

        clean(args.out)
